<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hoa Le - Project Portfolio</title>
    <link rel="stylesheet" href="styles-3.css">
  </head>

  <body>
    <header>
      <h1>PROJECT PORTFOLIO</h1>
    </header>
    
    <main>
      <div class="greeting">
        <p>Welcome to my portfolio website! My name is Hoa Le, and I go by Harvey.</p>

        <p>On the first day of 2024, I registered for the FRM Part 1 exam, embarking on a new career journey in 
          Financial Risk Management 
          with a focus on Credit Risk. I passed the exam in May 2024 with only 10 days of self-study 
          thanks to having a strategy tailored to my educational background, learning style, and schedule. I am eager to put 
          it to test again with my Part 2 exam coming in November.
        </p>

        <p>This website presents some projects I have worked on 
         in the domains of financial risk, data science, and process enhancement/improvement. Check them out! ðŸ‘‡
        </p>

      </div>

      
      <div class="portfolio">

        <div class="project">    
          <h3>Data Preprocessing for Mortgage Loan Default Prediction</h3>
          <hr>
          <div>
            <img class="project_img" src='/assets/img/project2.png' alt='histogram of features' >
          </div>
           
          <p>The objective of this project is to preprocess a large dataset of mortgage loan applications to improve data quality 
            and ensure the accuracy of the default prediction model. By handling missing values, outliers, and noisy data, the preprocessing 
            phase sets the stage for effective modeling and analysis.</p>
          <p>The dataset consists of approximately 150,000 mortgage loan applications submitted in 2019, containing 32 attributes. The 
            attributes include demographic information, loan characteristics, and financial indicators relevant to mortgage defaults.</p>
          <p>The dataset presented challenges such as missing values in certain attributes, outliers in income and loan amounts, 
            and noisy data that required cleansing before it could be used for modeling.</p>
          <p>I used data visualization tools such as boxplot, histogram, or heatmap to detect patterns in the data, visualize missing values, 
            and spot outliers. These visualizations helped to inform decisions on data transformation and feature engineering.</p>
          <p>I employed the K-Nearest Neighbors (KNN) imputation technique to handle missing values in numerical features. This method
            fills in missing data by using the average of the 'k' nearest neighbors, ensuring that the imputed values are similar 
            to existing data points. For categorical features, I decoded them with a new category 'missing'. In cases where missing values were extensive 
            and compromised data quality, I evaluated the necessity of the attribute and removed it if needed.</p>
          <p>After preprocessing, the dataset was free of missing values, outliers were handled appropriately, and noisy data was 
            removed. The processed dataset is now well-prepared for feature extraction and model development.</p>
          <p class="tech">Technologies: Python, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn</p>
          
          
          <form action="https://github.com/Hoale2908/Home-Lending-Default-Classification-Model/blob/main/model.ipynb">
            <input type="submit" value="Open Notebook" />
          </form>
        </div>

        <div class="project">
          <h3>Corporate Bond Default Prediction Using Machine Learning</h3>
          <hr>
          <div>
            <img class="project_img" src='/assets/img/corp_bond.png' alt='correlation heat map' >
          </div>
          
          <p>The objective of this project is to develop a machine learning model to predict the likelihood of corporate bond default. 
            This can help investors assess the credit risk of bond issuers and make informed decisions.</p>
          <p>I used historical financial data for corporate bonds, including balance sheet ratios and income statement ratios. 
            The dataset was from the period 1995-2004 from over 800 companies.</p>
          <p>I explored various models, including Logistic Regression, Random Forest, and Gradient Boosting, to predict bond defaults. 
            The models were trained on historical data, and performance was evaluated using accuracy, precision, recall, and the ROC-AUC score.</p>
          <p>One of the key challenges was dealing with class imbalance, as defaults are relatively rare events. 
            I addressed this by implementing oversampling techniques and using evaluation metrics that account for imbalance, such as precision-recall curves.</p>
          <p class="tech">Technologies: Python, Scikit-learn, Pandas, NumPy, Matplotlib</p>
            <form action="https://github.com/Hoale2908/Predicting-Company-Default">
          
              <input type="submit" value="Open Notebook" />
          </form>
        </div>

        
        
      </div>
      
      
    

    </main> 
  </body>
  
  <footer>
    
    <div class="social">
      <a href="https://www.linkedin.com/in/lehoa2908">LinkedIn</a>
      <a href="hoa.letp@gmail.com">Email</a>
      
    </div>

    
    <p class="date">Last update: Sep 22, 2024.</p>
    
  </footer>
</html>